{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Airflow DAG**"
      ],
      "metadata": {
        "id": "_jXY8xMj4M6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvcEqZPI3xXG"
      },
      "outputs": [],
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash import BashOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "with DAG(\n",
        "    dag_id='monthly_flights_etl',\n",
        "    default_args=default_args,\n",
        "    start_date=datetime(2025, 6, 15),\n",
        "    schedule_interval='@daily',\n",
        "    catchup=False,\n",
        "    tags=['batch', 'pyspark'],\n",
        ") as dag:\n",
        "\n",
        "    start = BashOperator(\n",
        "        task_id='start',\n",
        "        bash_command='echo \"Mulai ETL untuk Monthly Flights\"'\n",
        "    )\n",
        "\n",
        "    run_etl = BashOperator(\n",
        "    task_id='run_etl',\n",
        "    bash_command='spark-submit --jars /usr/local/airflow/jars/postgresql-42.6.0.jar /usr/local/airflow/jobs/process_monthly_flights.py'\n",
        ")\n",
        "\n",
        "\n",
        "    end = BashOperator(\n",
        "        task_id='end',\n",
        "        bash_command='echo \"ETL selesai dan data telah diproses\"'\n",
        "    )\n",
        "\n",
        "    start >> run_etl >> end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ETL dengan PySpark**"
      ],
      "metadata": {
        "id": "UZlc5BVs5MhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# === SPARK SESSION ===\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MonthlyFlightsETL\") \\\n",
        "    .config(\"spark.jars\", \"/usr/local/airflow/jars/postgresql-42.6.0.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# === EXTRACT ===\n",
        "df = spark.read.csv(\"/usr/local/airflow/data/monthly_flights.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# === TRANSFORM ===\n",
        "agg_df = df.groupBy(\"date\").sum(\"monthly_total_flights\") \\\n",
        "           .withColumnRenamed(\"sum(monthly_total_flights)\", \"total_flights\")\n",
        "\n",
        "# === 1. TAMPILKAN OUTPUT DI CONSOLE ===\n",
        "print(\"=== Hasil Ringkasan Monthly Flights ===\")\n",
        "agg_df.show(truncate=False)\n",
        "\n",
        "# === 2. SIMPAN KE FILE CSV ===\n",
        "agg_df.coalesce(1).write.csv(\"/usr/local/airflow/output/monthly_flights_summary\", header=True, mode=\"overwrite\")\n",
        "\n",
        "# === 3. SIMPAN KE POSTGRESQL ===\n",
        "agg_df.write.format(\"jdbc\").options(\n",
        "    url=\"jdbc:postgresql://localhost:5432/flightdb\",\n",
        "    driver=\"org.postgresql.Driver\",\n",
        "    dbtable=\"public.monthly_flights_summary\",\n",
        "    user=\"airflowuser\",\n",
        "    password=\"airflowpass\"\n",
        ").mode(\"overwrite\").save()\n",
        "\n",
        "# === STOP SPARK ===\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "Iq8tsHvn4ZUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}